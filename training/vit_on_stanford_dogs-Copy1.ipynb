{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13ee14ce-7e0a-4b3a-a69f-c81d66769619",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2025-01-08 16:55:41'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets\n",
    "from torchvision import models\n",
    "import os\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "import datetime\n",
    "datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a492112-f3e5-49e7-baf3-e5c46295762d",
   "metadata": {},
   "source": [
    "# HYPER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36b6258d-fcf0-472b-b4c0-b4e226428c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f0630b745d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c95c8702-7724-430f-92f3-aa837332c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"./data/Images\"\n",
    "#hyperparameter:\n",
    "learning_rate = 5e-5\n",
    "batch_size = 128\n",
    "#adam paras:\n",
    "betas=(0.9, 0.999), \n",
    "eps=1e-8\n",
    "\n",
    "training_steps = 1000\n",
    "num_classes = 120\n",
    "num_epochs = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8ac7fa8-2c9c-4753-b43d-73cc9a692aee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e3ca159-f43f-4daf-ac6b-d871cd831295",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6af4d30-0e9d-4e37-8bab-5945edec134c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "timm.data.IMAGENET_DEFAULT_MEAN,timm.data.IMAGENET_DEFAULT_STD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafbfe25-f569-4590-9966-c9299ae6e3f0",
   "metadata": {},
   "source": [
    "# DATA LOADER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc9205a1-48f4-46c5-84c1-d676c9f17985",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "\n",
    "trans_ = T.Compose([\n",
    "    T.Resize((224, 224)), \n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=timm.data.IMAGENET_DEFAULT_MEAN, std=timm.data.IMAGENET_DEFAULT_STD)\n",
    "])\n",
    "\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=trans_)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5655045d-ad4e-45ec-8a5b-9536551755d5",
   "metadata": {},
   "source": [
    "### 计算每个epoch的batch数为step数，每个step都会更新梯度，乘以epochs得到总的steps\n",
    "- = 20580 * 80% /128 = 128.625 向上取整\n",
    "- 总的steps = 129 * numepochs = 1935\n",
    "- 实际训练时可能会提前终止"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e4b8679e-d589-4358-b498-05ab48d59e5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "129"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1a5d4d65-cb1b-4e23-879a-48c126388df2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([128, 3, 224, 224]), torch.Size([128]), 1935)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_steps = len(train_loader) * num_epochs\n",
    "images, labels = next(iter(train_loader))\n",
    "images.size(),labels.size(),num_steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06aed3b-6061-4ff1-a022-47194592ca29",
   "metadata": {},
   "source": [
    "# MODEL_PRETRAINED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ebaa06b-2517-426a-8a69-214eff44b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at /workspaces and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTForImageClassification\n",
    "model_path = '/workspaces'\n",
    "\n",
    "model = ViTForImageClassification.from_pretrained(model_path,num_labels = num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "feb098da-7142-4561-95db-90361c04ab9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cef6a7-b152-4761-bba0-515f3d90ef17",
   "metadata": {},
   "source": [
    "# FINE_TUNING LOOP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e071ad4-cd53-4e3c-9e0b-8b2539a283e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c1526c00-473d-4f01-9a6e-709ab9a71efe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647f638-baf6-45e6-bbe5-abc9eaad8586",
   "metadata": {},
   "source": [
    "<!-- from transformers import Adafactor, Trainer, TrainingArguments\n",
    "\n",
    "# 定义训练参数\n",
    "training_args = TrainingArguments(\n",
    "    evaluation_strategy=\"steps\",            # 每隔一定步数进行评估\n",
    "    save_strategy=\"steps\",                  # 每隔一定步数保存模型\n",
    "    learning_rate=5e-5,                     # 学习率\n",
    "    gradient_accumulation_steps=4,          # 梯度累积\n",
    "    gradient_checkpointing=True,            # 启用梯度检查点\n",
    "    optim=\"adafactor\",                      # 使用 Adafactor 优化器\n",
    "    max_steps=1000,                          # 总训练步数\n",
    "    eval_delay=0,                           # 评估延迟\n",
    "    logging_steps=100,                      # 每100步进行一次日志记录\n",
    "    save_steps=200,                         # 每200步保存一次模型\n",
    "    load_best_model_at_end=True,            # 在训练结束时加载最佳模型\n",
    "    metric_for_best_model=\"f1\",             # 评估标准\n",
    "    greater_is_better=True,                 # F1 分数越高越好\n",
    "    report_to=\"mlflow\",                     # 将日志报告到 MLflow\n",
    "    save_total_limit=2,                     # 最多保存2个模型\n",
    "    output_dir = './output'\n",
    ")\n",
    "\n",
    "# 使用 Adafactor 优化器，指定学习率和 beta 值\n",
    "optimizer = Adafactor(\n",
    "    model.parameters(),        # 使用 training_args 中的学习率\n",
    "    eps=1e-8,                               # 防止数值问题\n",
    "    # betas=(0.9, 0.999),                     # beta 设置为（0.9， 0.999）\n",
    "    weight_decay=0.01,                      # 权重衰减\n",
    "    relative_step=True,                     # 使用相对步长\n",
    "    warmup_init=True                        # 启用热启动\n",
    ")\n",
    "\n",
    "# 使用 Trainer API 进行训练\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,            # 训练数据集\n",
    "    eval_dataset=val_dataset,              # 验证数据集\n",
    "    optimizers=(optimizer, None),           # 使用自定义优化器\n",
    ")\n",
    " -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fe9dd0-8b55-41fe-88c4-1c0de12b2a57",
   "metadata": {},
   "source": [
    "\n",
    "### 传统SGD：https://pytorch.org/docs/stable/generated/torch.optim.SGD.html \n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\eta \\nabla_\\theta J(\\theta_{t-1}),\n",
    "$$\n",
    "\n",
    "### Adam：https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
    "- **step1:**\n",
    "    $$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_\\theta J(\\theta_{t-1}),\n",
    "$$\n",
    "\n",
    "    $$\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) \\left(\\nabla_\\theta J(\\theta_{t-1})\\right)^2,\n",
    "$$\n",
    "\n",
    "- **step2:**\n",
    "\n",
    "    $$\n",
    "\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t},\n",
    "$$ \n",
    "- **step3:**\n",
    "\n",
    "    $$\n",
    "\\theta_t = \\theta_{t-1} - \\frac{\\eta \\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon},\n",
    "$$\n",
    "\n",
    "**优点：** Adam的算法可以简单的理解为RMSProp和动量优化的结合：\n",
    "- 其中动量优化提供了动态调整学习率的思路，可以有效缓和震荡问题\n",
    "- RMSProp可以理解为AdaGrad与指数加权移动平均算法的结合，其中：\n",
    "    - AdaGrad对于不同更新力度的参数定制不同的学习率\n",
    "    - 指数加权移动平均算法使AdaGrad梯度累加更加平滑，避免了早停"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbae23bc-a01d-42c0-8ef9-2c44701b0d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [1/129], Loss: 0.4783, Accuracy: 0.00%, 2025-01-08 16:55:50\n",
      "Epoch [1/15], Step [11/129], Loss: 4.7495, Accuracy: 3.55%, 2025-01-08 16:56:21\n",
      "Epoch [1/15], Step [21/129], Loss: 4.6840, Accuracy: 7.25%, 2025-01-08 16:56:53\n",
      "Epoch [1/15], Step [31/129], Loss: 4.6149, Accuracy: 11.59%, 2025-01-08 16:57:24\n",
      "Epoch [1/15], Step [41/129], Loss: 4.5167, Accuracy: 17.66%, 2025-01-08 16:57:54\n",
      "Epoch [1/15], Step [51/129], Loss: 4.4376, Accuracy: 23.58%, 2025-01-08 16:58:26\n",
      "Epoch [1/15], Step [61/129], Loss: 4.3386, Accuracy: 29.43%, 2025-01-08 16:58:56\n",
      "Epoch [1/15], Step [71/129], Loss: 4.2511, Accuracy: 34.00%, 2025-01-08 16:59:28\n",
      "Epoch [1/15], Step [81/129], Loss: 4.1620, Accuracy: 37.65%, 2025-01-08 16:59:58\n",
      "Epoch [1/15], Step [91/129], Loss: 4.0740, Accuracy: 40.79%, 2025-01-08 17:00:29\n",
      "Epoch [1/15], Step [101/129], Loss: 3.9692, Accuracy: 43.73%, 2025-01-08 17:01:01\n",
      "Epoch [1/15], Step [111/129], Loss: 3.8877, Accuracy: 46.14%, 2025-01-08 17:01:32\n",
      "Epoch [1/15], Step [121/129], Loss: 3.8073, Accuracy: 48.30%, 2025-01-08 17:02:03\n",
      "Epoch [1/15] Eval Loss: 3.6812, Eval Accuracy: 75.42%\n",
      "Epoch [2/15], Step [1/129], Loss: 0.3647, Accuracy: 81.25%, 2025-01-08 17:04:00\n",
      "Epoch [2/15], Step [11/129], Loss: 3.5956, Accuracy: 79.26%, 2025-01-08 17:04:31\n",
      "Epoch [2/15], Step [21/129], Loss: 3.5212, Accuracy: 79.99%, 2025-01-08 17:05:04\n",
      "Epoch [2/15], Step [31/129], Loss: 3.4304, Accuracy: 80.42%, 2025-01-08 17:05:35\n",
      "Epoch [2/15], Step [41/129], Loss: 3.3465, Accuracy: 80.41%, 2025-01-08 17:06:07\n",
      "Epoch [2/15], Step [51/129], Loss: 3.2551, Accuracy: 80.64%, 2025-01-08 17:06:38\n",
      "Epoch [2/15], Step [61/129], Loss: 3.2295, Accuracy: 80.46%, 2025-01-08 17:07:08\n",
      "Epoch [2/15], Step [71/129], Loss: 3.1467, Accuracy: 80.60%, 2025-01-08 17:07:39\n",
      "Epoch [2/15], Step [81/129], Loss: 3.0618, Accuracy: 80.71%, 2025-01-08 17:08:10\n",
      "Epoch [2/15], Step [91/129], Loss: 3.0018, Accuracy: 80.87%, 2025-01-08 17:08:41\n",
      "Epoch [2/15], Step [101/129], Loss: 2.9425, Accuracy: 80.92%, 2025-01-08 17:09:13\n",
      "Epoch [2/15], Step [111/129], Loss: 2.8612, Accuracy: 81.17%, 2025-01-08 17:09:44\n",
      "Epoch [2/15], Step [121/129], Loss: 2.8075, Accuracy: 81.30%, 2025-01-08 17:10:14\n",
      "Epoch [2/15] Eval Loss: 2.7696, Eval Accuracy: 79.96%\n",
      "Epoch [3/15], Step [1/129], Loss: 0.2576, Accuracy: 89.06%, 2025-01-08 17:12:13\n",
      "Epoch [3/15], Step [11/129], Loss: 2.5710, Accuracy: 89.42%, 2025-01-08 17:12:44\n",
      "Epoch [3/15], Step [21/129], Loss: 2.5173, Accuracy: 89.10%, 2025-01-08 17:13:15\n",
      "Epoch [3/15], Step [31/129], Loss: 2.4567, Accuracy: 88.79%, 2025-01-08 17:13:45\n",
      "Epoch [3/15], Step [41/129], Loss: 2.3914, Accuracy: 88.53%, 2025-01-08 17:14:16\n",
      "Epoch [3/15], Step [51/129], Loss: 2.3436, Accuracy: 88.33%, 2025-01-08 17:14:47\n",
      "Epoch [3/15], Step [61/129], Loss: 2.2678, Accuracy: 88.50%, 2025-01-08 17:15:18\n",
      "Epoch [3/15], Step [71/129], Loss: 2.2171, Accuracy: 88.39%, 2025-01-08 17:15:49\n",
      "Epoch [3/15], Step [81/129], Loss: 2.1205, Accuracy: 88.69%, 2025-01-08 17:16:21\n",
      "Epoch [3/15], Step [91/129], Loss: 2.1197, Accuracy: 88.64%, 2025-01-08 17:16:52\n",
      "Epoch [3/15], Step [101/129], Loss: 2.0700, Accuracy: 88.66%, 2025-01-08 17:17:24\n",
      "Epoch [3/15], Step [111/129], Loss: 2.0177, Accuracy: 88.74%, 2025-01-08 17:17:55\n",
      "Epoch [3/15], Step [121/129], Loss: 1.9622, Accuracy: 88.69%, 2025-01-08 17:18:26\n",
      "Epoch [3/15] Eval Loss: 2.0688, Eval Accuracy: 82.58%\n",
      "Epoch [4/15], Step [1/129], Loss: 0.1750, Accuracy: 92.19%, 2025-01-08 17:20:21\n",
      "Epoch [4/15], Step [11/129], Loss: 1.7481, Accuracy: 92.40%, 2025-01-08 17:20:52\n",
      "Epoch [4/15], Step [21/129], Loss: 1.7002, Accuracy: 92.30%, 2025-01-08 17:21:22\n",
      "Epoch [4/15], Step [31/129], Loss: 1.6742, Accuracy: 92.06%, 2025-01-08 17:21:52\n",
      "Epoch [4/15], Step [41/129], Loss: 1.6380, Accuracy: 92.17%, 2025-01-08 17:22:22\n",
      "Epoch [4/15], Step [51/129], Loss: 1.5874, Accuracy: 91.96%, 2025-01-08 17:22:53\n",
      "Epoch [4/15], Step [61/129], Loss: 1.5719, Accuracy: 91.92%, 2025-01-08 17:23:26\n",
      "Epoch [4/15], Step [71/129], Loss: 1.4978, Accuracy: 92.13%, 2025-01-08 17:23:58\n",
      "Epoch [4/15], Step [81/129], Loss: 1.4694, Accuracy: 92.20%, 2025-01-08 17:24:29\n",
      "Epoch [4/15], Step [91/129], Loss: 1.4294, Accuracy: 92.19%, 2025-01-08 17:24:59\n",
      "Epoch [4/15], Step [101/129], Loss: 1.3898, Accuracy: 92.41%, 2025-01-08 17:25:31\n",
      "Epoch [4/15], Step [111/129], Loss: 1.3642, Accuracy: 92.34%, 2025-01-08 17:26:02\n",
      "Epoch [4/15], Step [121/129], Loss: 1.3241, Accuracy: 92.36%, 2025-01-08 17:26:33\n",
      "Epoch [4/15] Eval Loss: 1.5778, Eval Accuracy: 84.04%\n",
      "Epoch [5/15], Step [1/129], Loss: 0.1211, Accuracy: 97.66%, 2025-01-08 17:28:29\n",
      "Epoch [5/15], Step [11/129], Loss: 1.1916, Accuracy: 95.10%, 2025-01-08 17:29:01\n",
      "Epoch [5/15], Step [21/129], Loss: 1.1537, Accuracy: 95.16%, 2025-01-08 17:29:33\n",
      "Epoch [5/15], Step [31/129], Loss: 1.1428, Accuracy: 94.81%, 2025-01-08 17:30:05\n",
      "Epoch [5/15], Step [41/129], Loss: 1.0663, Accuracy: 95.03%, 2025-01-08 17:30:35\n",
      "Epoch [5/15], Step [51/129], Loss: 1.0725, Accuracy: 94.99%, 2025-01-08 17:31:06\n",
      "Epoch [5/15], Step [61/129], Loss: 1.0278, Accuracy: 95.07%, 2025-01-08 17:31:37\n",
      "Epoch [5/15], Step [71/129], Loss: 1.0117, Accuracy: 95.21%, 2025-01-08 17:32:08\n",
      "Epoch [5/15], Step [81/129], Loss: 0.9798, Accuracy: 95.26%, 2025-01-08 17:32:40\n",
      "Epoch [5/15], Step [91/129], Loss: 0.9537, Accuracy: 95.32%, 2025-01-08 17:33:13\n",
      "Epoch [5/15], Step [101/129], Loss: 0.9612, Accuracy: 95.21%, 2025-01-08 17:33:43\n",
      "Epoch [5/15], Step [111/129], Loss: 0.8985, Accuracy: 95.22%, 2025-01-08 17:34:13\n",
      "Epoch [5/15], Step [121/129], Loss: 0.8999, Accuracy: 95.15%, 2025-01-08 17:34:45\n",
      "Epoch [5/15] Eval Loss: 1.2795, Eval Accuracy: 84.50%\n",
      "Epoch [6/15], Step [1/129], Loss: 0.0815, Accuracy: 97.66%, 2025-01-08 17:36:42\n",
      "Epoch [6/15], Step [11/129], Loss: 0.8204, Accuracy: 97.37%, 2025-01-08 17:37:14\n",
      "Epoch [6/15], Step [21/129], Loss: 0.7850, Accuracy: 97.02%, 2025-01-08 17:37:44\n",
      "Epoch [6/15], Step [31/129], Loss: 0.7862, Accuracy: 96.93%, 2025-01-08 17:38:16\n"
     ]
    }
   ],
   "source": [
    "# train_loop\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(), \n",
    "    lr=5e-5, \n",
    "    betas=betas, \n",
    "    eps=eps\n",
    ")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 定义学习率调度器\n",
    "def lr_lambda(current_step: int):\n",
    "    return max(0.0, 1.0 - current_step / num_steps)\n",
    "\n",
    "lr_scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "total_steps = len(train_loader)  # 每个epoch的训练步数\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train() \n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for step, (images, labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs.logits, labels) \n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs.logits, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "        lr_scheduler.step()\n",
    "\n",
    "        if step % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{step+1}/{total_steps}], Loss: {running_loss/10:.4f}, Accuracy: {100 * correct/total:.2f}%, {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    model.eval()  # 设置模型为评估模式\n",
    "    eval_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():  \n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs.logits, labels)  \n",
    "            eval_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs.logits, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    eval_loss /= len(val_loader)\n",
    "    eval_acc = correct / total * 100\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] Eval Loss: {eval_loss:.4f}, Eval Accuracy: {eval_acc:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c570b91a-f1bc-4b6d-9a4f-3182dbb4349d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!poweroff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0878c039-8629-4e94-9888-37eec3150edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"vit_finetuned_StanfordDogs_ep5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda07042-c614-4213-a4fc-1474b207cea4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
